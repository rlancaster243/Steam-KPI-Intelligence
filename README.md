# ğŸ® Steam KPI Intelligence (SKI)
### Predictive Analytics & Lifecycle Management for Steam Game Success

> *"Predicting what makes a game win, before it launches."*

---

## ğŸ§© Overview

**Steam KPI Intelligence (SKI)** is a complete **data science and MLOps framework** that predicts the commercial performance of games on the Steam platform.  
It unites data engineering, machine learning, explainable AI, and web deployment into one cohesive analytical system.

The goal is simple: **forecast player adoption and success probability** from pricing, engagement, and sentiment metrics â€” and visualize the results interactively.

---

## ğŸš€ Key Objectives

- Predict **game ownership (log-scaled)** using regression models  
- Classify **top-decile success probability** using ensemble learning  
- Explain **model behavior** using SHAP, LIME, and permutation importance  
- Deploy an interactive **Dash web app** for real-time scenario testing  
- Automate **data drift detection** and **model retraining**

---

## ğŸ“Š Problem Statement

Game studios, publishers, and investors face uncertainty when estimating how a game will perform after release.  
SKI addresses this by using historical Steam data to **quantify the relationship between pricing, engagement, and success** â€” giving decision-makers predictive insight.

---

## ğŸ§  Methodology

### 1ï¸âƒ£ Data Wrangling & Feature Engineering

Processed and standardized **86,500+ Steam game records**.  
Created derived variables capturing pricing, ownership, engagement, and sentiment behavior.

Key engineered features include:
- `owners_mid`, `votes_total`, `sentiment_ratio`
- `price_delta`, `engagement_rate_2w`, `price_per_hour`

Each numeric field was normalized and validated to remove outliers and ensure schema consistency.

---

### 2ï¸âƒ£ Modeling Framework

- **Regression Target:** `log(owners_mid)`  
- **Classification Target:** Binary indicator for top-decile ownership  
- **Algorithms Used:**
  - `ElasticNetCV`
  - `GradientBoostingRegressor`
  - `LogisticRegressionCV`
  - `GradientBoostingClassifier`

Feature selection applied:
- Variance thresholding  
- Correlation pruning  
- L1-regularization (LASSO)

---

### 3ï¸âƒ£ Explainability & Interpretability

- **Permutation Importance:** Global feature contribution  
- **SHAP & LIME:** Local interpretability  
- **Decision Tree Surrogates:** Approximation for decision transparency  

**Top predictive drivers:**
- `votes_total` (review count)  
- `price_delta` (discount magnitude)  
- `average_2weeks_hours` (recent engagement)  
- `average_forever_hours` (lifetime engagement)

---

### 4ï¸âƒ£ Interactive Dash Deployment

Deployed a responsive **Dash** web app for simulation and reporting.

**App Tabs:**
- ğŸ¯ *Single Scenario* â€” Interactive prediction  
- ğŸ“ *Batch Scoring* â€” Bulk CSV scoring  
- ğŸ” *Insights* â€” Model transparency and metadata display  

Includes a **Price Ã— Discount Heatmap** for revenue optimization analysis.

---

### 5ï¸âƒ£ Drift Monitoring & Retraining

- Tracks data drift via feature distribution divergence  
- Auto-rebuilds engineered features and targets on detection  
- Saves model versions in `/artifacts/model_history`  
- Logs retraining metadata in `heartbeat_log.csv`

---

### 6ï¸âƒ£ Lifecycle Automation

**APScheduler** automates:
- Metric logging  
- Drift checks  
- Retraining triggers  

Regression metrics: RMSE, MAE, RÂ²  
Classification metrics: ROC-AUC, F1, Brier Score

---

## ğŸ“ˆ Results

| Task | Model | Metric | Value |
|------|--------|--------|--------|
| Regression | GradientBoostingRegressor | RMSE | **1.188** |
| Regression | ElasticNetCV | RÂ² | **0.081** |
| Classification | GradientBoostingClassifier | ROC-AUC | **0.944** |
| Classification | GradientBoostingClassifier | Accuracy | **0.953** |
| Classification | LogisticCV | F1-Score | **0.731** |

**Interpretation:**  
Moderate discounts and strong engagement metrics correlate with ownership growth, while excessive discounts reduce revenue efficiency.

---

## âš™ï¸ Technical Stack

- **Language:** Python 3.12  
- **Libraries:** `pandas`, `numpy`, `scikit-learn`, `shap`, `lime`, `plotly`, `dash`, `joblib`, `apscheduler`  
- **Environment:** Local + deployable on Render or Heroku  
- **Version Control:** Git with timestamped model artifacts  
- **MLOps Components:** Drift detection, retraining, explainability, web deployment

---

# ğŸ–¥ï¸ How to Use the Dashboard

Once launched, open your browser at:  
ğŸ‘‰ **http://127.0.0.1:8051**

The dashboard allows interactive simulation, bulk scoring, and interpretability review.  
It contains three main tabs: **Single Scenario**, **Batch Scoring**, and **Insights**.

---

### ğŸ¯ Single Scenario Tab

Simulate the launch of a single game by entering its KPIs.  
Each field corresponds to a measurable feature from the dataset.

| Feature | Description | Typical Range |
|----------|--------------|----------------|
| `price` | Current sale price (in cents) | 0 â€“ 10,000 |
| `initialprice` | Original price before discounts | 0 â€“ 10,000 |
| `discount` | Discount percentage | 0 â€“ 100 % |
| `userscore` | Steam user score | 0 â€“ 100 |
| `votes_total` | Total reviews (positive + negative) | 0 â€“ 1,000,000+ |
| `sentiment` | Positive review ratio | 0 â€“ 1 |
| `ccu` | Concurrent users (current) | 0 â€“ 100,000+ |
| `average_forever_hours` | Mean lifetime playtime | 0 â€“ 100+ |
| `average_2weeks_hours` | Mean recent playtime | 0 â€“ 50+ |
| `median_forever_hours` | Median lifetime playtime | 0 â€“ 100+ |
| `price_delta` | Difference between original and sale price | Variable |
| `price_per_hour` | Price divided by playtime | 0 â€“ 5,000 |
| `engagement_rate_2w` | Active share of players (2 weeks) | 0 â€“ 1 |

After entering values, click **â€œPredict Single Scenario.â€**

**Outputs:**

- **Owners (pred):** Predicted number of game owners  
- **Success Prob:** Probability of being in the top 10 % of games  
- **Decision Badge:** PASS if above classification threshold, else FAIL  

---

#### ğŸ”¥ Price Ã— Discount Heatmap

Visualizes how **price** and **discount** jointly affect projected revenue (`price Ã— predicted owners`).

- **X-Axis:** Discount percentage  
- **Y-Axis:** Price  
- **Color Gradient:** Brighter = higher expected revenue  

**Interpretation:**

- Bright zones identify optimal price-discount combinations  
- Moderate discounts at mid-range prices often yield peak performance  
- Excessive discounts (>80 %) reduce total revenue despite volume gains  

---

### ğŸ“ Batch Scoring Tab

Score multiple titles simultaneously â€” ideal for publishers and analysts.

**Steps:**

1. Upload a `.csv` file containing all model features.  
2. Click **â€œScore File.â€**  
3. The app:
   - Validates and reorders feature columns  
   - Applies consistent preprocessing  
   - Runs both regression and classification models  
   - Computes a `revenue_proxy` metric  
4. Preview top 200 records directly in-app  
5. Use **â€œDownload Predictionsâ€** to export full results  

**Output Columns:**

| Column | Meaning |
|---------|----------|
| `owners_pred` | Predicted total owners |
| `success_prob` | Probability of top-decile success |
| `revenue_proxy` | Price Ã— predicted owners |
| _Original features_ | Retained for auditing and analysis |

---

### ğŸ” Insights Tab

Provides transparency and governance for deployed models.

**Sections:**

1. **Global Importance (Regression):**  
   - Top 15 predictors of ownership (`y_reg_log_owners`)
2. **Global Importance (Classification):**  
   - Top 15 predictors of success probability (`y_clf_success_top10`)
3. **Deployed Thresholds:**  
   - JSON metadata including cutoff probabilities, feature quantiles, and version info

**Purpose:**  
Understand which KPIs drive model outcomes and maintain trust in deployed predictions.

---

### ğŸ’¡ Practical Use Cases

- **Developers:** Optimize pricing and promotion strategy  
- **Publishers:** Evaluate portfolio performance  
- **Investors:** Forecast ROI potential  
- **Analysts:** Monitor engagement and market sentiment trends  

---

### ğŸ§­ Recommended Workflow

1. Explore **Single Scenario** for hypothesis testing  
2. Use **Batch Scoring** for large-scale evaluation  
3. Review **Insights** for interpretability and model audit  
4. Export results for integration into internal BI pipelines  

---

## ğŸ§© Outputs

| Artifact | Description |
|-----------|--------------|
| `model_reg.pkl`, `model_clf.pkl` | Trained model artifacts |
| `importance_reg_permutation.csv` | Feature importance results |
| `app_dash.py` | Dash web app |
| `drift_report.csv`, `heartbeat_log.csv` | Drift and monitoring logs |
| `model_history/` | Versioned retraining archive |

---

## ğŸ§­ Impact

**Steam KPI Intelligence (SKI)** demonstrates a full machine-learning lifecycle â€” from exploration to automated deployment â€” highlighting mastery in:

- Quantitative modeling  
- Data science engineering  
- MLOps automation  
- Model interpretability  
- Full-stack deployment with Dash  

---

## ğŸ‘¨â€ğŸ’» Developer

**Russell I. Lancaster**  
_MSc in Financial Engineering | Data Scientist_  
**Waukesha, Wisconsin**

ğŸ“§ `Russell.Lancaster243@gmail.com`  
ğŸ”— [LinkedIn: Russell Lancaster](https://www.linkedin.com/in/rlancaster243)

---

